{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var nb = IPython.notebook;\n",
       "var kernel = IPython.notebook.kernel;\n",
       "var command = \"NOTEBOOK_FULL_PATH = '\" + nb.notebook_path + \"'\";\n",
       "kernel.execute(command);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "var nb = IPython.notebook;\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var command = \"NOTEBOOK_FULL_PATH = '\" + nb.notebook_path + \"'\";\n",
    "kernel.execute(command);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Problem\n",
    "\n",
    "A Car insurance company wants to predict the severity of an accident based on the features of the car and other usage pattern of the applicant. The accurate prediction will help the company categorise users based on the chances of an applicant getting into severe accidents and further decision making like identifying high risk/low risk customers and offering a customized insurance premium to them.\n",
    "\n",
    "The company will have the following features of the vehicle and details of the applicant in during the application process. The goal is to create a predictive model for the company using these features to predict accident severity.\n",
    "\n",
    "**Independent Variables**:\n",
    "\n",
    "1. Vehicle Type\n",
    "2. Journey purpose of the driver\n",
    "3. Sex of the driver\n",
    "4. Age of the driver\n",
    "5. Engine capacity. of the vehicle (cc)\n",
    "6. Propulsion code\n",
    "7. Age of vehicle\n",
    "8. Generic make model of the vehicle\n",
    "9. Driver's IMD decile\n",
    "10. Driver's Home area type\n",
    "11. Vehicle left hand drive\n",
    "\n",
    "**Dependent Variable**: Accident Severity\\\n",
    "\n",
    "\n",
    "\n",
    "**Dataset Information**:\n",
    "\n",
    "We are going to use `Accident` and `Vehicle` data of the year 2020 from the website of govt of UK from [UK Road Safety Data](https://data.gov.uk/dataset/cb7ae6f0-4be6-4935-9277-47e5ce24a11f/road-safety-data \"Click to see the source\"). Firt we will select the relevant columns from both the data and then we will merge the two using the column `accident_index` which is a unique value for each accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 1482\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from nbformat import read, NO_CONVERT\n",
    "\n",
    "with io.open(NOTEBOOK_FULL_PATH.split(\"/\")[-1], 'r', encoding='utf-8') as f:\n",
    "    nb = read(f, NO_CONVERT)\n",
    "\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print(f\"Word count: {word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting logging to print only error messages from Sklearnex\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"SKLEARNEX\").setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(palette=\"Set2\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None) #to see all the columns of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "\n",
    "# urls to download the data\n",
    "URL1 = \"https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-accident-2020.csv\"\n",
    "URL2 = \"https://data.dft.gov.uk/road-accidents-safety-data/dft-road-casualty-statistics-vehicle-2020.csv\"\n",
    "URL3 = \"https://data.dft.gov.uk/road-accidents-safety-data/Road-Safety-Open-Dataset-Data-Guide.xlsx\"\n",
    "\n",
    "#function to load the dataframes\n",
    "def get_dataframe():\n",
    "    global URL1,URL2\n",
    "    \n",
    "    # if the \"datasets\" folder does not exist, create it\n",
    "    if not os.path.exists(\"datasets\"):\n",
    "        os.makedirs(\"datasets\")\n",
    "    \n",
    "    # if the accident file does not exist, download it\n",
    "    if not os.path.exists(\"datasets/accident2020.csv\"):\n",
    "        urllib.request.urlretrieve(URL1, \"datasets/accident2020.csv\")\n",
    "     \n",
    "    # if the vehicle file does not exist, download it\n",
    "    if not os.path.exists(\"datasets/vehicle2020.csv\"):\n",
    "        urllib.request.urlretrieve(URL2, \"datasets/vehicle2020.csv\")   \n",
    "        \n",
    "    # if the schema file does not exist, download it\n",
    "    if not os.path.exists(\"datasets/schema.csv\"):\n",
    "        urllib.request.urlretrieve(URL3, \"datasets/schema.xlsx\")\n",
    "        \n",
    "    ac = pd.read_csv(\"datasets/accident2020.csv\")\n",
    "    vh = pd.read_csv(\"datasets/vehicle2020.csv\")\n",
    "    schema = pd.read_excel(\"datasets/schema.xlsx\")\n",
    "    \n",
    "    # load the dataframe\n",
    "    return ac,vh,schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "ac,vh,schema = get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking accident df\n",
    "ac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking vehicle df\n",
    "vh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the schema\n",
    "schema.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting field name as index to acess the fields easily\n",
    "\n",
    "schema.set_index(\"field name\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example after setting index\n",
    "schema.loc[\"vehicle_reference\"].note[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarly we can access the values from the schema to understand the data more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking shape of both dataframes\n",
    "\n",
    "print(\"Shape of the Accident dataframe is\", ac.shape)\n",
    "print(\"Shape of the Vehicle dataframe is\", vh.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of rows in Vehicle dataset is more than 1.5 times of the same in Accident dataset. Lets explore more to see what is causing it since both the datasets are from the year 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking unique accident index in ac & vh\n",
    "\n",
    "if ac.accident_index.nunique() == ac.shape[0]:\n",
    "    print(\"All values in accident_index are unique in Accident dataframe.\")\n",
    "else:\n",
    "    print(\"All values in accident_index are not unique in Accident dataframe.\")\n",
    "    \n",
    "if vh.accident_index.nunique() == vh.shape[0]:\n",
    "    print(\"All values in accident_index are unique in Vehicle dataframe.\")\n",
    "else:\n",
    "    print(\"All values in accident_index are not unique in Vehicle dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This means we have multiple records of each accident in the vehicle data. Lets look at both the dataframes to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking initial rows of ac\n",
    "ac.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking initial rows of vh\n",
    "vh.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By looking at both the dataframes we can see that some of the accident index has multiple rows in vehicle. We can see the vehicle reference has multiple values for a single accident index, which shows the details of multiple vehicles involved a single accident. This increases the number of rows in the vehicle dataset which is good for our problem statement as we will be able to use different vehicle data while training the predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting required columns\n",
    "\n",
    "#Selecting required cols from accident \n",
    "ac = ac[['accident_index','longitude',\n",
    "        'latitude','accident_severity']]\n",
    "\n",
    "#selecting required cols from vehicle\n",
    "vh = vh[['accident_index','vehicle_type','journey_purpose_of_driver','sex_of_driver',\n",
    "       'age_of_driver','age_band_of_driver','engine_capacity_cc',\n",
    "        'propulsion_code','age_of_vehicle','driver_home_area_type','driver_imd_decile',\n",
    "        'vehicle_left_hand_drive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting vh type only for cars\n",
    "\n",
    "vh.vehicle_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the schema to understand the types\n",
    "\n",
    "schema.loc[\"vehicle_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vh.vehicle_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The vehicle type 9 has the highest number of observations which is what we need for the analysis.\n",
    "\n",
    "- We could have merged type 108 and 109 too which also represent cars but we don't have records for those type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting data only for car\n",
    "\n",
    "vh = vh[vh[\"vehicle_type\"] == 9]\n",
    "vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vh.vehicle_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We don't need the variable vehicle_type anymore since all the records belong to car now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping vehicle_type\n",
    "\n",
    "vh.drop(\"vehicle_type\",axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape after dropping\n",
    "vh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging two data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the dataframes\n",
    "import functools as ft\n",
    "\n",
    "dfs = [ac,vh]\n",
    "\n",
    "df_merged = ft.reduce(lambda left, right: pd.merge(left, right, on='accident_index'), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Insepction\n",
    "\n",
    "Let's look at the schema which explains each of the variable in out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the target variable\n",
    "\n",
    "schema.loc[\"accident_severity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#journey_purpose_of_driver\n",
    "\n",
    "schema.loc['journey_purpose_of_driver']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver_home_area_type\n",
    "schema.loc[\"driver_home_area_type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we see '-1' represents data missing or out of range in many variables. These must be removed as these are not valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's remove all rows having -1\n",
    "\n",
    "df_merged = df_merged.replace(-1, np.nan).dropna(axis=0)\n",
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Many categorical variables have been stored as integer/float type. Most of them are represented by numbers and these are not ordinal.We are going to replace those numbers with the respective category as described in the schema. It will also help in unique columns after we change these to dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing label for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to change labels for categorical variables\n",
    "\n",
    "def change_label(col):\n",
    "    df_merged[col] = df_merged[col].replace(schema.loc[col].set_index('code/format')['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #changing labels for categorical variables\n",
    "\n",
    "cat_cols = ['accident_severity','journey_purpose_of_driver','sex_of_driver','age_band_of_driver',\n",
    "        'propulsion_code','driver_home_area_type','driver_imd_decile',\n",
    "        'vehicle_left_hand_drive']\n",
    "#df_merged[cols] = df_merged[cols].astype('object')\n",
    "\n",
    "for col in cat_cols:\n",
    "    change_label(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged[cols] = df_merged[cols].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping accident_index as it is not required anymore\n",
    "df_merged.drop('accident_index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will cluster the lattitude and longitude data to divide into different locations and will add a new variable named location to use it as a predictor.\n",
    "\n",
    "- We will divide the whole of UK into 4 regions and see if it makes sense to add an additional variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means clustering of longitude and latitude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting latitude and longitude in a new dataframe\n",
    "df = df_merged[['latitude','longitude']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Calculating the Hopkins statistic\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# from random import sample\n",
    "# from numpy.random import uniform\n",
    "# import numpy as np\n",
    "# from math import isnan\n",
    " \n",
    "# def hopkins(X):\n",
    "#     d = X.shape[1]\n",
    "#     #d = len(vars) # columns\n",
    "#     n = len(X) # rows\n",
    "#     m = int(0.1 * n) \n",
    "#     nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n",
    " \n",
    "#     rand_X = sample(range(0, n, 1), m)\n",
    " \n",
    "#     ujd = []\n",
    "#     wjd = []\n",
    "#     for j in range(0, m):\n",
    "#         u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n",
    "#         ujd.append(u_dist[0][1])\n",
    "#         w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n",
    "#         wjd.append(w_dist[0][1])\n",
    " \n",
    "#     H = sum(ujd) / (sum(ujd) + sum(wjd))\n",
    "#     if isnan(H):\n",
    "#         print(ujd, wjd)\n",
    "#         H = 0\n",
    " \n",
    "#     return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hopkins(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# #Lets do the silhouette score analysis to find the right number of clusters\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# sse_ = []\n",
    "# for k in range(2, 5):\n",
    "#     kmeans = KMeans(n_clusters=k).fit(df)\n",
    "#     sse_.append([k, silhouette_score(df, kmeans.labels_)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #plotting silhouette_score vs number of clusters\n",
    "# plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The analysis shows 3 is the right number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means using K=3\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "model_clus3 = KMeans(n_clusters = 3, max_iter=50,random_state = 50)\n",
    "model_clus3.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning Cluster IDs\n",
    "\n",
    "df.index = pd.RangeIndex(len(df.index))\n",
    "df = pd.concat([df, pd.Series(model_clus3.labels_)], axis=1)\n",
    "df.columns = ['latitude', 'longitude','cluster_id']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking counts\n",
    "df.cluster_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing data type\n",
    "df.cluster_id = df.cluster_id.astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting clusters\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(df, x='longitude', y='latitude',color='cluster_id')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As per the above plot it is fair to divide the whole of UK into 3 different regions. The Insurance company can get the location from the customers and can place them in the appropriate category.\n",
    "\n",
    "- Cluster 1 represents south-east, 2 represents south west, and 0 represents rest of UK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging clusterid to original data frame\n",
    "\n",
    "df_merged['location'] = df['cluster_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# changing location id to region names\n",
    "df_merged['location'] = df_merged['location'].map({0:'Rest of UK',1:'South-east',\n",
    "                                                   2:'South-west'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_merged, x='longitude', y='latitude',color='location')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking count of the target variable\n",
    "df_merged.accident_severity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the set with similar proportion of accident_severity\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df,test_df = train_test_split(df_merged, test_size=0.2, random_state=7,\n",
    "                                    stratify=df_merged[\"accident_severity\"])\n",
    "\n",
    "# train_df,test_df = train_test_split(df_merged, test_size=0.2, random_state=7,\n",
    "#                                       stratify=df_merged[\"accident_severity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{train_df.shape[0]} train and {test_df.shape[0]} test instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #using pandas profiling for initial exploration\n",
    "# from pandas_profiling import ProfileReport\n",
    "# prof_rep = ProfileReport(train_df)\n",
    "# prof_rep.to_file(output_file='output.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prof_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights from Pandas Profile Report\n",
    "\n",
    "- Target variable accident_severity is highly imbalanced.\n",
    "- Around 57% of rows in journey purpose of driver is unknown. We will have to clean it.\n",
    "- Around 80% of drivers are from urban area. This might have a effect  which we will have to explore further.\n",
    "- High correlations found among some variables which we will look separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the distribution of numeric variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The maximum age of driver is 98 which is very unlikely. We need to address this while cleaning.\n",
    "\n",
    "- There is a huge gap between the minimum,maximum engine capacity and the median one. We will adress this in the data cleaning process.\n",
    "- We are observing extreme values for age_of_vehicle too. Lets visualize all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function to create distribution of a continuous variable\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.offline as pyo\n",
    "\n",
    "def create_hist(var, group_label, bin_):\n",
    "    \n",
    "    fig = ff.create_distplot([var],group_labels = [group_label], curve_type = 'normal', \n",
    "                             bin_size = bin_)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Visualizing age of driver\n",
    "\n",
    "# create_hist(train_df['age_of_driver'], 'Age of Driver',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimum age is 11 in the dataset and some observations are less than 16 which is the legal age in UK. These must be removed.\n",
    "- Some observations for age are close to 100, which we should address during data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Visualizing engine_capacity_cc\n",
    "\n",
    "# create_hist(train_df['engine_capacity_cc'], 'Engine CApacity',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Visualizing age of vehicle\n",
    "\n",
    "# create_hist(train_df['age_of_vehicle'], 'Age of Vehicle',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the vehicles are less than 20 years of age. Some extreme values are also present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-variate analysis (Numeric variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function to plot scatter plots showing relationship between two continuous variables\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "def create_scat(df, var1, var2):\n",
    "    fig = px.scatter(df, x=var1, y=var2)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Comparing applicant_income & loan_amount\n",
    "\n",
    "# create_scat(train_df,'age_of_driver','age_of_vehicle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The age of the driver and age of vehicle are not related. The data is evenly spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #plotting longitude and lattitude to see the  spread of accidents across location\n",
    "# px.set_mapbox_access_token(\"pk.eyJ1Ijoic2FzaGlrYW50NyIsImEiOiJjbDRmbjFpOWswMG01M2RranZ4b2VrZDgxIn0.IDHMdhaaKIT73LTDRLym2A\")\n",
    "# fig = px.scatter_mapbox(train_df, lat=\"latitude\", lon=\"longitude\", color=\"accident_severity\",\n",
    "#                   color_continuous_scale=px.colors.cyclical.IceFire, size_max=20,zoom=4)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the observations in our data point are from England. By zooming in we can see that observations from Wales and Scotland are very few.\n",
    "\n",
    "- Different severity types are evenly spread across UK and our predictive model will be able to generalize well across UK which suits our requirement in the business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function for categorical variables analysis\n",
    "\n",
    "def cat_plot(col):\n",
    "    print(train_df[col].value_counts()/len(train_df[col])*100) # % of categories\n",
    "    \n",
    "    fig = px.histogram(train_df, x=col,color='accident_severity',histnorm='percent',text_auto=True)\n",
    "    fig.update_layout(barmode='group', xaxis={'categoryorder':'total descending'})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#checking journey_purpose_of_driver\n",
    "cat_plot('journey_purpose_of_driver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 56.73% journey purpose is not known. We can not fix this so we will have to drop the whole column.\n",
    "- It is interesting to see that in 62% of the fatal cases the journey purpose is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#checking sex_of_driver\n",
    "cat_plot('sex_of_driver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In 71.8% of the fatal accident cases the driver is a male.\n",
    "- Female are morelikely to be involved in a slight accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#checking age_band_of_driver\n",
    "cat_plot('age_band_of_driver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In 24.69% of the accidents, age of the driver is between 26-35. \n",
    "\n",
    "- It is noticable that the age group 'over 75' is more involved in fatal accidents.\n",
    "\n",
    "- We have few variables in the age group od 11-15 and 6-10. These are outiers which we must remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#checking propulsion_code\n",
    "cat_plot('propulsion_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Petrol cars are involved in 57.64% of the total accidents. 57.91% of fatal accidents also caused by petrol cars.\n",
    "\n",
    "- Other than Pentrol and diesel there are very few items for other fuel types. We will merge these to one group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#checking driver_imd_decile\n",
    "cat_plot('driver_imd_decile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is quite evident that drivers from less deprived areas cause more fatal accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#checking driver_home_area_type\n",
    "cat_plot('driver_home_area_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Around 80% of drivers involved in an accident come from urban areas. \n",
    "\n",
    "- Rural area drivers are more likely to get involved in a fatal accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#checking vehicle_left_hand_drive\n",
    "cat_plot('vehicle_left_hand_drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 95% of the cars have right hand drive. It is better to drop the whole column since it doesn't add much variance to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking vehicle_left_hand_drive\n",
    "cat_plot('location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Proportion of fatal accidents are more in South-west and Rest of the UK while it is less in the south east region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking target variable 'accident_severity'\n",
    "\n",
    "fig = px.histogram(train_df, x='accident_severity',histnorm='percent',text_auto=True)\n",
    "fig.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a huge class imbalance in our dataset. Number of accidents with slight category is the highest and fatal is the lowest. We should balance the dataset before the modelling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis for all variables\n",
    "\n",
    "In pandas profile report we identified that there are some variables in our data set which are highly correlated. We will idenrify those using the **Phik (φk)** which is a new and practical correlation coefficient that works consistently between categorical, ordinal and interval variables, captures non-linear dependency and reverts to the Pearson correlation coefficient in case of a bivariate normal input distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phik\n",
    "from phik import resources, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the phi_k correlation matrix between all variables\n",
    "corr = train_df.phik_matrix()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "corr = corr.mask(mask)\n",
    "corr.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting a heatmat\n",
    "fig = px.imshow(corr, text_auto=True,aspect=\"auto\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The variables age and age group of driver are highly correlated. We will drop age group of driver.Longitude and latitude are highly correlated too. We will drop both as we have already converted those to location column.\n",
    "\n",
    "- Some other pairs like vehicle_left_hand_drive and sex_of_driver , make and engine_capacity, make and propulsion code have positive correlations but those are onder 0.60. So we will keep those.\n",
    "\n",
    "- There is no risk of multicollinearity once we drop age_band_of_driver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Transformation\n",
    "\n",
    "Apart from adressing all the data isuues identified in the EDA stage, we will also look for other data issues and try to fix those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking null values\n",
    "\n",
    "print(\"Sum of null values in training set is:\",train_df.isna().sum())\n",
    "print(\"Sum of null values in testing set is:\",train_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping journey_purpose_of_driver,age_band_of_driver',\n",
    "#'vehicle_left_hand_drive', 'longitude' & 'latitude' as decided in the EDA stage\n",
    "\n",
    "#dropping from train\n",
    "train_df.drop(['journey_purpose_of_driver','age_band_of_driver','longitude',\n",
    "               'latitude','vehicle_left_hand_drive'],axis=1,inplace=True)\n",
    "\n",
    "#dropping from test\n",
    "test_df.drop(['journey_purpose_of_driver','age_band_of_driver','longitude',\n",
    "               'latitude','vehicle_left_hand_drive'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged.propulsion_code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining hybrid types to other in propulsion_code\n",
    "vals = ['Hybrid electric', 'Petrol/Gas (LPG)','Gas/Bi-fuel', 'Electric diesel',\n",
    "       'Gas', 'Gas Diesel']\n",
    "\n",
    "train_df['propulsion_code'] = train_df['propulsion_code'].replace(vals,'Hybrid')\n",
    "test_df['propulsion_code'] = test_df['propulsion_code'].replace(vals,'Hybrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot side by side box plot\n",
    "\n",
    "def create_box(col):\n",
    "    \n",
    "    fig = px.box(train_df, x=\"accident_severity\", y=col,color=\"sex_of_driver\")\n",
    "    fig.update_traces(quartilemethod=\"exclusive\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #plotting age of driver against accident severity\n",
    "\n",
    "# create_box(\"age_of_driver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can spot outliers in the Slight category for age more than 80. Since there is no upper age limit in the UK to drive a car, we will not remove these but we will transform the variable. We have already seen the distribution is right skewed in the eda stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transforming variable 'age_of_driver' on base 10\n",
    "\n",
    "train_df['age_of_driver'] = np.log10(train_df['age_of_driver'])\n",
    "test_df['age_of_driver'] = np.log10(test_df['age_of_driver'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plotting age of vehicle against accident severity\n",
    "# create_box(\"age_of_vehicle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are ovious outliers present in the variable age_of_vehicle. As an insurance company we will have to cap these values as too old cars can not be good for the business as it will have more maintenance. So we will cap the values at 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing rows  age_of_vehicle > 30\n",
    "\n",
    "train_df = train_df[train_df['age_of_vehicle'] < 30]\n",
    "test_df = test_df[test_df['age_of_vehicle'] < 30]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The variable engine_capacity_cc is also right skewed and we will log transform it to keep the distribution normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transforming variable 'engine_capacity_cc' on base 10\n",
    "\n",
    "train_df['engine_capacity_cc'] = np.log10(train_df['engine_capacity_cc'])\n",
    "test_df['engine_capacity_cc'] = np.log10(test_df['engine_capacity_cc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting all categorical columns in one list\n",
    "\n",
    "cat_cols = [x for x in train_df.select_dtypes(\"object\").columns]\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dummy variables for all categorical columns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "for c in cat_cols:\n",
    "    \n",
    "    if c != 'accident_severity': #excluding target variable\n",
    "        \n",
    "        #create a nee encoder for each category\n",
    "        one_hot_encoder = OneHotEncoder(drop='first',handle_unknown='ignore',sparse=False)\n",
    "\n",
    "        # the input to the encoder must be a 2-d numpy array,\n",
    "        # so we take the column, extract their values and reshape the array to be 2-d\n",
    "        cat_vals = train_df[c].values.reshape(-1,1)\n",
    "\n",
    "        transformed = one_hot_encoder.fit_transform(cat_vals)\n",
    "\n",
    "        # put the transformed data as columns in the dataframe\n",
    "        col_names = one_hot_encoder.categories_[0].tolist()[1:]\n",
    "        for i, col_name in enumerate(col_names):\n",
    "            train_df[col_name] = transformed[:,i]\n",
    "\n",
    "        #transforming testset with fitted encoder\n",
    "        cat_vals = test_df[c].values.reshape(-1,1)\n",
    "        transformed = one_hot_encoder.transform(cat_vals)\n",
    "\n",
    "        for i, col_name in enumerate(col_names):\n",
    "            test_df[col_name] = transformed[:,i]    \n",
    "\n",
    "        #Delete original categorical columns\n",
    "        train_df.drop(c,axis=1,inplace=True)\n",
    "        test_df.drop(c,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# train_target = train_df[\"median_house_value\"].values\n",
    "# train_predictors = train_df.drop(\"median_house_value\", axis=1)\n",
    "\n",
    "# # fit_transform returns a NumPy array, so we need to put it back \n",
    "# # into a Pandas dataframe\n",
    "# scaled_vals = scaler.fit_transform(trainset_predictors)\n",
    "# trainset = pd.DataFrame(scaled_vals, columns=trainset_predictors.columns)\n",
    "\n",
    "# # put the non-scaled target back in\n",
    "# trainset['median_house_value'] = trainset_target\n",
    "\n",
    "# # inspect the data\n",
    "# trainset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling train_df columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "num_cols = [x for x in train_df.select_dtypes(\"float64\").columns ]\n",
    "\n",
    "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "\n",
    "test_df[num_cols] = scaler.transform(test_df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.rename(columns={'Not known': 'Other_gender'}, inplace=True)\n",
    "test_df.rename(columns={'Not known': 'Other_gender'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveing the final data in an excel\n",
    "#df.to_excel(\"final_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveing the final data in an excel\n",
    "# train_df.to_excel(\"testset.xlsx\")\n",
    "# test_df.to_excel(\"trainset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from nbformat import read, NO_CONVERT\n",
    "\n",
    "with io.open(NOTEBOOK_FULL_PATH.split(\"/\")[-1], 'r', encoding='utf-8') as f:\n",
    "    nb = read(f, NO_CONVERT)\n",
    "\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print(f\"Word count: {word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
